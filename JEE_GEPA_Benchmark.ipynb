{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13076426",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b9ab7f",
   "metadata": {},
   "source": [
    "# GEPA for JEE (Math)\n",
    "Using the `dspy.GEPA` optimizer, we compare the performances of Gemma 27b It and OpenAI OSS 20b on Math questions from JEE, before and after GEPA optmization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283588ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = input(\"Enter your OpenAI API key: \")\n",
    "import dspy\n",
    "import re\n",
    "\n",
    "api_key = \"your_api_key\"\n",
    "gemini_api_key = \"your_api_key\"\n",
    "\n",
    "gemma = \"openrouter/google/gemma-3-27b-it\"\n",
    "qwen = \"openrouter/qwen/qwen3-32b\"\n",
    "openai_oss = \"openai/gpt-oss-20b\"\n",
    "\n",
    "      \n",
    "extraction_lm = dspy.LM(\n",
    "    model=\"openrouter/openai/gpt-4o-mini\", \n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key, \n",
    "    temperature=0.1, \n",
    "    max_tokens=32000\n",
    ")\n",
    "\n",
    "from dspy.adapters import TwoStepAdapter\n",
    "adapter = dspy.adapters.TwoStepAdapter(extraction_lm)\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=gemma,\n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key,\n",
    "    temperature=1,\n",
    "    max_tokens=32000,\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm, adapter=adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fac24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Define a signature for feedback\n",
    "class FeedbackSignature(dspy.Signature):\n",
    "    \"\"\"Give feedback on an incorrect solution to a math problem. It might be incorrect due to the reasoning or the answer formatting,\n",
    "    solution must be inside \\boxed{}. solution must follow this format: problem = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "    \"\"\"\n",
    "    question: str = dspy.InputField()\n",
    "    solution: str = dspy.InputField()\n",
    "    expected_answer: str = dspy.InputField()\n",
    "    feedback: str = dspy.OutputField(desc=\"Constructive feedback about the solution\")\n",
    "\n",
    "gemini_lm = dspy.LM(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=32000,\n",
    "    api_key=gemini_api_key\n",
    ")\n",
    "\n",
    "# Function to get feedback\n",
    "def get_llm_feedback(question, solution, expected_answer):\n",
    "    # Create the feedback module\n",
    "    feedback_module = dspy.ChainOfThought(\n",
    "        FeedbackSignature,\n",
    "        lm=gemini_lm\n",
    "    )\n",
    "    \n",
    "    with dspy.context(lm=gemini_lm):\n",
    "        # Call the LLM with structured inputs\n",
    "        result = feedback_module(\n",
    "            question=question,\n",
    "            solution=solution,\n",
    "            expected_answer=expected_answer\n",
    "    )\n",
    "        \n",
    "    print(\"Feedback:\", result.feedback[:100])\n",
    "    \n",
    "    return result.feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llm_feedback(\"How much is 2 + 2?\", \"The answer is 4.\", \"4\")  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac06351",
   "metadata": {},
   "outputs": [],
   "source": [
    "math = dspy.ChainOfThought(\"question -> answer: float\")\n",
    "math(question=\"Two dice are tossed. What is the probability that the sum equals 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb853f2c",
   "metadata": {},
   "source": [
    "### Loading the JEE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "\n",
    "def init_jee_dataset():\n",
    "    dataset = load_dataset(\"PhysicsWallahAI/JEE-Main-2025-Math\", \"apr\")['test']\n",
    "\n",
    "    # Split the dataset\n",
    "    split = dataset.train_test_split(test_size=0.25, seed=42)\n",
    "    train_set_2025 = split['train']\n",
    "    test_set_2025 = split['test'].select(range(int(0.5 * len(split['test']))))\n",
    "    val_set_2025 = split['test'].select(range(int(0.5 * len(split['test'])), len(split['test'])))\n",
    "\n",
    "    # Convert to dspy.Example format\n",
    "    train_set = [\n",
    "        dspy.Example({\n",
    "            \"problem\": x['question'],\n",
    "            \"answer\": x['answer'],\n",
    "        }).with_inputs(\"problem\")\n",
    "        for x in train_set_2025\n",
    "    ]\n",
    "\n",
    "    val_set = [\n",
    "        dspy.Example({\n",
    "            \"problem\": x['question'],\n",
    "            \"answer\": x['answer'],\n",
    "        }).with_inputs(\"problem\")\n",
    "        for x in val_set_2025\n",
    "    ]\n",
    "\n",
    "    test_set = [\n",
    "        dspy.Example({\n",
    "            \"problem\": x['question'],\n",
    "            \"answer\": x['answer'],\n",
    "        }).with_inputs(\"problem\")\n",
    "        for x in test_set_2025\n",
    "    ]\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Example usage\n",
    "train, val, test = init_jee_dataset()\n",
    "print(\"Train size:\", len(train))\n",
    "print(\"Val size:\", len(val))\n",
    "print(\"Test size:\", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552769fd",
   "metadata": {},
   "source": [
    "### Defining the program: `dspy.ChainOfThought`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71598add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateResponse(dspy.Signature):\n",
    "    \"\"\"Solve the problem and provide the answer in the correct format.\"\"\"\n",
    "    problem = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "program = dspy.ChainOfThought(GenerateResponse)\n",
    "program_openai = dspy.ChainOfThought(GenerateResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e795e1b",
   "metadata": {},
   "source": [
    "### Defining the evaluation metric\n",
    "We simply check exact match between the predicted answer and the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    correct_answer = str(example['answer'])\n",
    "    try:\n",
    "        llm_answer = str(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        return 0\n",
    "    return int(correct_answer == llm_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb9bae",
   "metadata": {},
   "source": [
    "### Optimize the program with `dspy.GEPA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimental: adding a LLm-as-judge to provide feedback\n",
    "def metric_with_feedback_llm(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    correct_answer = str(example['answer'])\n",
    "    written_solution = example.get('solution', '')\n",
    "    try:\n",
    "        llm_answer = str(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        print(f\"Couldn't parse answer as integer: {prediction.answer}\")\n",
    "        feedback_text = f\"The final answer must be a valid integer and nothing else. You responded with '{prediction.answer}', which couldn't be parsed as a python integer. Please ensure your answer is a valid integer without any additional text or formatting.\"\n",
    "        #feedback_text += f\" The correct answer is '{correct_answer}'.\"\n",
    "        if written_solution:\n",
    "        #    feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems and ensure your final answer is a valid integer.\"\n",
    "            feedback_text += get_llm_feedback(example['problem'], written_solution, str(correct_answer))\n",
    "        return dspy.Prediction(score=0, feedback=feedback_text)\n",
    "\n",
    "    score = int(correct_answer == llm_answer)\n",
    "\n",
    "    feedback_text = get_llm_feedback(example['problem'], written_solution, str(correct_answer))\n",
    "    # feedback_text = \"\"\n",
    "    # if score == 1:\n",
    "    #     feedback_text = f\"Your answer is correct. The correct answer is '{correct_answer}'.\"\n",
    "    # else:\n",
    "    #     feedback_text = f\"Your answer is incorrect. The correct answer is '{correct_answer}'.\"\n",
    "    \n",
    "    # if written_solution:\n",
    "    #     feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems.\"\n",
    "\n",
    "    return dspy.Prediction(score=score, feedback=feedback_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_with_feedback(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    \n",
    "    correct_answer = str(example['answer'])\n",
    "    written_solution = example.get('solution', '')\n",
    "    try:\n",
    "        llm_answer = str(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        feedback_text = f\"The final answer must be a valid integer and nothing else. You responded with '{prediction.answer}', which couldn't be parsed as a python integer. Please ensure your answer is a valid integer without any additional text or formatting.\"\n",
    "        feedback_text += f\" The correct answer is '{correct_answer}'.\"\n",
    "        if written_solution:\n",
    "            feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems and ensure your final answer is a valid integer.\"\n",
    "        return dspy.Prediction(score=0, feedback=feedback_text)\n",
    "    \n",
    "    if llm_answer is None:\n",
    "        feedback_text = f\"The final answer must be a valid integer and nothing else. You responded with '{prediction.answer}', which couldn't be parsed as a python integer. Please ensure your answer is a valid integer without any additional text or formatting.\"\n",
    "        feedback_text += f\" The correct answer is '{correct_answer}'.\"\n",
    "        if written_solution:\n",
    "            feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems and ensure your final answer is a valid integer.\"\n",
    "        return dspy.Prediction(score=0, feedback=feedback_text)\n",
    "\n",
    "    score = int(correct_answer == llm_answer)\n",
    "\n",
    "    # if not score and pred_name is not None:\n",
    "    #     print(\"Calling the llm feedback function!\")\n",
    "    #     return metric_with_feedback_llm(example, prediction, trace, pred_name, pred_trace)\n",
    "    # else:\n",
    "    feedback_text = \"\"\n",
    "    if score == 1:\n",
    "        feedback_text = f\"Your answer is correct. The correct answer is '{correct_answer}'.\"\n",
    "    else:\n",
    "        feedback_text = f\"Your answer is incorrect. The correct answer is '{correct_answer}'.\"\n",
    "    \n",
    "    if written_solution:\n",
    "        feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems.\"\n",
    "\n",
    "    return dspy.Prediction(score=score, feedback=feedback_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract candidate data with prompts and scores for analysis\n",
    "if hasattr(optimized_program_openai, 'detailed_results'):\n",
    "    results = optimized_program_openai.detailed_results\n",
    "    \n",
    "    # Create structured candidate data\n",
    "    candidate_data = []\n",
    "    \n",
    "    for i, (candidate, score) in enumerate(zip(results.candidates, results.val_aggregate_scores)):\n",
    "        for name, predictor in candidate.named_predictors():\n",
    "            prompt = predictor.signature.instructions\n",
    "            \n",
    "            candidate_entry = {\n",
    "                'candidate_idx': i,\n",
    "                'score': score,\n",
    "                'prompt': prompt,\n",
    "                'prompt_length_chars': len(prompt),\n",
    "                'prompt_length_words': len(prompt.split()),\n",
    "                'predictor_name': name\n",
    "            }\n",
    "            \n",
    "            candidate_data.append(candidate_entry)\n",
    "            break  # Just take first predictor for each candidate\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"ðŸ“‹ GEPA CANDIDATE DATA EXTRACTED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total candidates: {len(candidate_data)}\")\n",
    "    \n",
    "    # Show best and worst performers\n",
    "    best_candidate = max(candidate_data, key=lambda x: x['score'])\n",
    "    worst_candidate = min(candidate_data, key=lambda x: x['score'])\n",
    "    \n",
    "    print(f\"\\nðŸ† Best candidate:\")\n",
    "    print(f\"  Index: {best_candidate['candidate_idx']}\")\n",
    "    print(f\"  Score: {best_candidate['score']:.4f}\")\n",
    "    print(f\"  Length: {best_candidate['prompt_length_chars']} chars, {best_candidate['prompt_length_words']} words\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‰ Worst candidate:\")\n",
    "    print(f\"  Index: {worst_candidate['candidate_idx']}\")\n",
    "    print(f\"  Score: {worst_candidate['score']:.4f}\")\n",
    "    print(f\"  Length: {worst_candidate['prompt_length_chars']} chars, {worst_candidate['prompt_length_words']} words\")\n",
    "    \n",
    "    # Show score progression\n",
    "    scores = [c['score'] for c in candidate_data]\n",
    "    print(f\"\\nðŸ“ˆ Score progression: {scores[0]:.3f} â†’ {scores[-1]:.3f} (change: {scores[-1] - scores[0]:+.3f})\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Usage examples:\")\n",
    "    print(f\"  Best prompt: candidate_data[{best_candidate['candidate_idx']}]['prompt']\")\n",
    "    print(f\"  All scores: [c['score'] for c in candidate_data]\")\n",
    "    print(f\"  High performers: [c for c in candidate_data if c['score'] > 0.5]\")\n",
    "    \n",
    "else:\n",
    "    print(\"No detailed results found. Make sure GEPA was run with track_stats=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data_openai.json\", \"w\") as json_file:\n",
    "    json.dump(candidate_data, json_file, indent=4)  # indent makes it more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c994a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine candidate_data structure for ancestry tracking\n",
    "print(f\"Total candidates: {len(candidate_data)}\")\n",
    "print(f\"First candidate keys: {list(candidate_data[0].keys())}\")\n",
    "print(f\"Sample candidate structure:\")\n",
    "import json\n",
    "print(json.dumps(candidate_data[0], indent=2)[:1000] + \"...\" if len(str(candidate_data[0])) > 1000 else json.dumps(candidate_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"light\",\n",
    "    #max_full_evals=2,\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=5,\n",
    "    reflection_lm=dspy.LM(model=\"gemini-2.5-pro\", temperature=1.0, max_tokens=32000, api_key=gemini_api_key)\n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train,\n",
    "    valset=val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476baff",
   "metadata": {},
   "source": [
    "It can be seen that what GEPA is doing here, is precomputing some reasoning to come up with a good plan for future task instances. Due to the improved performance in unseen validation set, we expect this prompt to generalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=test,\n",
    "    metric=metric,\n",
    "    num_threads=1,\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    "    provide_traceback=True\n",
    ")\n",
    "\n",
    "evaluate(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6459bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=test,\n",
    "    metric=metric,\n",
    "    num_threads=1,\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    "    provide_traceback=True\n",
    ")\n",
    "\n",
    "evaluate(optimized_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1975de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running GEPA with track_stats=True\n",
    "if hasattr(optimized_program, 'detailed_results'):\n",
    "    results = optimized_program.detailed_results\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    prompt_chars = []\n",
    "    prompt_words = []\n",
    "    scores = []\n",
    "    candidate_nums = []\n",
    "    \n",
    "    for i, (candidate, score) in enumerate(zip(results.candidates, results.val_aggregate_scores)):\n",
    "        for name, predictor in candidate.named_predictors():\n",
    "            prompt = predictor.signature.instructions\n",
    "            prompt_chars.append(len(prompt))\n",
    "            prompt_words.append(len(prompt.split()))\n",
    "            scores.append(score)\n",
    "            candidate_nums.append(i)\n",
    "            break  # Just take first predictor\n",
    "    \n",
    "    # Create the plots\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Score vs Character Count\n",
    "    scatter1 = ax1.scatter(prompt_chars, scores, c=candidate_nums, cmap='viridis', alpha=0.7, s=60)\n",
    "    ax1.set_xlabel('Prompt Length (characters)')\n",
    "    ax1.set_ylabel('Validation Score')\n",
    "    ax1.set_title('GEPA: Score vs Prompt Character Count')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    if len(prompt_chars) > 1:\n",
    "        correlation_chars = np.corrcoef(prompt_chars, scores)[0, 1]\n",
    "        ax1.text(0.05, 0.95, f'Correlation: {correlation_chars:.3f}', \n",
    "                transform=ax1.transAxes, \n",
    "                bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Add best candidate annotation\n",
    "    best_idx = results.best_idx\n",
    "    best_char_count = prompt_chars[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "    ax1.scatter([best_char_count], [best_score], c='red', s=100, marker='*', \n",
    "               label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Score vs Word Count\n",
    "    scatter2 = ax2.scatter(prompt_words, scores, c=candidate_nums, cmap='viridis', alpha=0.7, s=60)\n",
    "    ax2.set_xlabel('Prompt Length (words)')\n",
    "    ax2.set_ylabel('Validation Score')\n",
    "    ax2.set_title('GEPA: Score vs Prompt Word Count')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    if len(prompt_words) > 1:\n",
    "        correlation_words = np.corrcoef(prompt_words, scores)[0, 1]\n",
    "        ax2.text(0.05, 0.95, f'Correlation: {correlation_words:.3f}', \n",
    "                transform=ax2.transAxes, \n",
    "                bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Add best candidate annotation\n",
    "    best_word_count = prompt_words[best_idx]\n",
    "    ax2.scatter([best_word_count], [best_score], c='red', s=100, marker='*', \n",
    "               label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot 3: Evolution Timeline - Character Count\n",
    "    ax3.plot(candidate_nums, prompt_chars, 'b-o', markersize=4, alpha=0.7)\n",
    "    ax3.scatter([best_idx], [best_char_count], c='red', s=100, marker='*', \n",
    "               label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "    ax3.set_xlabel('Candidate Number (Evolution Order)')\n",
    "    ax3.set_ylabel('Prompt Length (characters)')\n",
    "    ax3.set_title('Prompt Length Evolution Over Time')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot 4: Evolution Timeline - Scores\n",
    "    ax4.plot(candidate_nums, scores, 'g-o', markersize=4, alpha=0.7)\n",
    "    ax4.scatter([best_idx], [best_score], c='red', s=100, marker='*', \n",
    "               label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "    ax4.set_xlabel('Candidate Number (Evolution Order)')\n",
    "    ax4.set_ylabel('Validation Score')\n",
    "    ax4.set_title('Score Evolution Over Time')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add colorbar for candidate numbers\n",
    "    plt.colorbar(scatter1, ax=ax1, label='Candidate #')\n",
    "    plt.colorbar(scatter2, ax=ax2, label='Candidate #')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GEPA PROMPT EVOLUTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nPrompt Length Statistics:\")\n",
    "    print(f\"Character count range: {min(prompt_chars)} - {max(prompt_chars)} chars\")\n",
    "    print(f\"Word count range: {min(prompt_words)} - {max(prompt_words)} words\")\n",
    "    print(f\"Average character count: {np.mean(prompt_chars):.1f} chars\")\n",
    "    print(f\"Average word count: {np.mean(prompt_words):.1f} words\")\n",
    "    \n",
    "    print(f\"\\nScore Statistics:\")\n",
    "    print(f\"Score range: {min(scores):.4f} - {max(scores):.4f}\")\n",
    "    print(f\"Average score: {np.mean(scores):.4f}\")\n",
    "    print(f\"Score std dev: {np.std(scores):.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Candidate:\")\n",
    "    print(f\"Candidate #{best_idx}: Score={best_score:.4f}\")\n",
    "    print(f\"Length: {best_char_count} chars, {best_word_count} words\")\n",
    "    \n",
    "    if len(prompt_chars) > 1:\n",
    "        print(f\"\\nCorrelations:\")\n",
    "        print(f\"Length (chars) vs Score: {correlation_chars:.3f}\")\n",
    "        print(f\"Length (words) vs Score: {correlation_words:.3f}\")\n",
    "        \n",
    "        # Growth analysis\n",
    "        initial_chars = prompt_chars[0]\n",
    "        final_chars = prompt_chars[-1]\n",
    "        char_growth = final_chars - initial_chars\n",
    "        char_growth_pct = (char_growth / initial_chars * 100) if initial_chars > 0 else 0\n",
    "        \n",
    "        initial_score = scores[0]\n",
    "        final_score = scores[-1]\n",
    "        score_change = final_score - initial_score\n",
    "        \n",
    "        print(f\"\\nEvolution Summary:\")\n",
    "        print(f\"Character growth: {initial_chars} â†’ {final_chars} ({char_growth:+d} chars, {char_growth_pct:+.1f}%)\")\n",
    "        print(f\"Score change: {initial_score:.4f} â†’ {final_score:.4f} ({score_change:+.4f})\")\n",
    "else:\n",
    "    print(\"No detailed results found. Make sure to run GEPA with track_stats=True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
