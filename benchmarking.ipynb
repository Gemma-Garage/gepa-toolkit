{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1c9b2b",
   "metadata": {},
   "source": [
    "# AIME GEPA Benchmarking Suite\n",
    "\n",
    "Comprehensive benchmarking system for GEPA optimization on AIME math problems.\n",
    "Features plug-and-play model configuration, automated evaluation, data export, and visualization.\n",
    "\n",
    "## Features\n",
    "- üîå **Plug-and-Play Models**: Easy model switching (OpenAI, Gemini, Claude, etc.)\n",
    "- üìä **Comprehensive Analysis**: Score evolution, prompt length tracking, correlation analysis\n",
    "- üíæ **Data Export**: JSON export with evolution trees and candidate metadata\n",
    "- üìà **Rich Visualizations**: 4-panel evolution plots inspired by index.ipynb\n",
    "- üéØ **AIME Specialized**: Optimized for mathematical competition problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Dependencies\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any, TypedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AnyMaths Adapter Dependencies\n",
    "import litellm\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Set up plotting style (inspired by index.ipynb)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6052ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnyMaths Adapter Implementation (from GEPA repository)\n",
    "class EvaluationBatch(TypedDict):\n",
    "    \"\"\"Results from evaluating a batch of examples.\"\"\"\n",
    "    outputs: List[Any]\n",
    "    scores: List[float]\n",
    "    trajectories: Optional[List[Any]]\n",
    "\n",
    "class AnyMathsDataInst(TypedDict):\n",
    "    input: str\n",
    "    additional_context: dict[str, str]\n",
    "    answer: str\n",
    "\n",
    "class AnyMathsTrajectory(TypedDict):\n",
    "    data: AnyMathsDataInst\n",
    "    full_assistant_response: str\n",
    "\n",
    "class AnyMathsRolloutOutput(TypedDict):\n",
    "    full_assistant_response: str\n",
    "\n",
    "class AnyMathsStructuredOutput(BaseModel):\n",
    "    final_answer: str = Field(\n",
    "        ..., description=\"The final answer to the mathematical problem (i.e., no units, no other text)\"\n",
    "    )\n",
    "    solution_pad: str = Field(..., description=\"The solution pad containing the step-by-step solution to the problem.\")\n",
    "\n",
    "class AnyMathsAdapter:\n",
    "    \"\"\"AnyMaths Adapter for mathematical word problems using LiteLLM.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        failure_score: float = 0.0,\n",
    "        api_base: str | None = None,\n",
    "        max_litellm_workers: int = 4,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.failure_score = failure_score\n",
    "        self.litellm = litellm\n",
    "        self.max_litellm_workers = max_litellm_workers\n",
    "        self.api_base = api_base\n",
    "\n",
    "        if self.model.startswith(\"ollama\"):\n",
    "            if self.api_base is None:\n",
    "                self.api_base = \"http://localhost:11434\"\n",
    "            assert self.api_base is not None, \"API base URL must be provided when using Ollama.\"\n",
    "\n",
    "        if self.api_base is None or self.api_base == \"\":\n",
    "            self.api_base = None\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        batch: List[AnyMathsDataInst],\n",
    "        candidate: dict[str, str],\n",
    "        capture_traces: bool = False,\n",
    "    ) -> EvaluationBatch:\n",
    "        import ast\n",
    "\n",
    "        outputs: List[AnyMathsRolloutOutput] = []\n",
    "        scores: List[float] = []\n",
    "        trajectories: List[AnyMathsTrajectory] | None = [] if capture_traces else None\n",
    "\n",
    "        if not candidate:\n",
    "            raise ValueError(\"Candidate must contain at least one component text.\")\n",
    "\n",
    "        system_content = next(iter(candidate.values()))\n",
    "\n",
    "        litellm_requests = []\n",
    "\n",
    "        for data in batch:\n",
    "            user_content = f\"{data['input']}\"\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ]\n",
    "\n",
    "            litellm_requests.append(messages)\n",
    "\n",
    "        try:\n",
    "            responses = self.litellm.batch_completion(\n",
    "                model=self.model,\n",
    "                messages=litellm_requests,\n",
    "                api_base=self.api_base,\n",
    "                max_workers=self.max_litellm_workers,\n",
    "                format=AnyMathsStructuredOutput.model_json_schema(),\n",
    "                response_format={\n",
    "                    \"type\": \"json_object\",\n",
    "                    \"response_schema\": AnyMathsStructuredOutput.model_json_schema(),\n",
    "                    \"enforce_validation\": True,\n",
    "                },\n",
    "            )\n",
    "        except litellm.exceptions.JSONSchemaValidationError as e:\n",
    "            raise e\n",
    "\n",
    "        for data, response in zip(batch, responses, strict=False):\n",
    "            correct_output_format = True\n",
    "            try:\n",
    "                assistant_response = ast.literal_eval(response.choices[0].message.content.strip())\n",
    "            except Exception:\n",
    "                assistant_response = \"Assistant failed to respond with the correct answer or format.\"\n",
    "                correct_output_format = False\n",
    "\n",
    "            if correct_output_format:\n",
    "                structured_assistant_response = f\"Assistant's Solution: {assistant_response['solution_pad']}\\n\"\n",
    "                structured_assistant_response += f\"Final Answer: {assistant_response['final_answer']}\"\n",
    "                output = {\"full_assistant_response\": structured_assistant_response}\n",
    "                score = 1.0 if data[\"answer\"] in assistant_response[\"final_answer\"] else self.failure_score\n",
    "            else:\n",
    "                output = {\"full_assistant_response\": assistant_response}\n",
    "                score = self.failure_score\n",
    "\n",
    "            outputs.append(output)\n",
    "            scores.append(score)\n",
    "\n",
    "            if capture_traces:\n",
    "                trajectories.append({\"data\": data, \"full_assistant_response\": output[\"full_assistant_response\"]})\n",
    "        \n",
    "        return EvaluationBatch(outputs=outputs, scores=scores, trajectories=trajectories)\n",
    "    \n",
    "    def make_reflective_dataset(\n",
    "        self,\n",
    "        candidate: dict[str, str],\n",
    "        eval_batch: EvaluationBatch,\n",
    "        components_to_update: list[str],\n",
    "    ) -> dict[str, list[dict[str, Any]]]:\n",
    "        ret_d: dict[str, list[dict[str, Any]]] = {}\n",
    "\n",
    "        assert len(components_to_update) == 1\n",
    "        comp = components_to_update[0]\n",
    "\n",
    "        items: list[dict[str, Any]] = []\n",
    "        trace_instances = list(zip(eval_batch['trajectories'], eval_batch['scores'], eval_batch['outputs'], strict=False))\n",
    "\n",
    "        for trace_instance in trace_instances:\n",
    "            traj, score, _ = trace_instance\n",
    "            data = traj[\"data\"]\n",
    "            generated_outputs = traj[\"full_assistant_response\"]\n",
    "\n",
    "            if score > 0.0:\n",
    "                feedback = f\"The generated response is correct. The final answer is: {data['answer']}.\"\n",
    "            else:\n",
    "                additional_context_str = \"\\n\".join(f\"{k}: {v}\" for k, v in data[\"additional_context\"].items())\n",
    "                if additional_context_str:\n",
    "                    feedback = (\n",
    "                        f\"The generated response is incorrect. The correct answer is: {data['answer']}. \"\n",
    "                        \"Ensure that the correct answer is included in the response exactly as it is. \"\n",
    "                        f\"Here is some additional context that might be helpful:\\n{additional_context_str}\"\n",
    "                    )\n",
    "                else:\n",
    "                    feedback = (\n",
    "                        f\"The generated response is incorrect. The correct answer is: {data['answer']}. \"\n",
    "                        \"Ensure that the correct answer is included in the response exactly as it is.\"\n",
    "                    )\n",
    "\n",
    "            d = {\"Inputs\": data[\"input\"], \"Generated Outputs\": generated_outputs, \"Feedback\": feedback}\n",
    "            items.append(d)\n",
    "\n",
    "        ret_d[comp] = items\n",
    "\n",
    "        if len(items) == 0:\n",
    "            raise Exception(\"No valid predictions found for any module.\")\n",
    "\n",
    "        return ret_d\n",
    "\n",
    "print(\"‚úÖ AnyMaths Adapter implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aime_datasets():\n",
    "    \"\"\"Load AIME datasets for training, validation, and testing.\"\"\"\n",
    "    \n",
    "    # Load AIMO validation set for training/validation\n",
    "    train_split = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n",
    "    train_data = []\n",
    "    for x in train_split:\n",
    "        # Convert to AnyMaths format\n",
    "        aime_data = {\n",
    "            \"input\": x['problem'],\n",
    "            \"additional_context\": {\"solution\": x.get('solution', '')},\n",
    "            \"answer\": str(x['answer']),\n",
    "        }\n",
    "        train_data.append(aime_data)\n",
    "    \n",
    "    # Shuffle and split\n",
    "    import random\n",
    "    random.Random(42).shuffle(train_data)\n",
    "    tot_num = len(train_data)\n",
    "\n",
    "    # Load AIME 2025 for testing\n",
    "    test_split = load_dataset(\"MathArena/aime_2025\")['train']\n",
    "    test_data = []\n",
    "    for x in test_split:\n",
    "        # Convert to AnyMaths format\n",
    "        aime_data = {\n",
    "            \"input\": x['problem'],\n",
    "            \"additional_context\": {},\n",
    "            \"answer\": str(x['answer']),\n",
    "        }\n",
    "        test_data.append(aime_data)\n",
    "\n",
    "    # Create splits\n",
    "    train_set = train_data[:int(0.5 * tot_num)]\n",
    "    val_set = train_data[int(0.5 * tot_num):]\n",
    "    test_set = test_data\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def extract_int(answer: str):\n",
    "    \"\"\"Try to parse an integer directly, or extract from \\\\boxed{}.\"\"\"\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return int(answer)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Try regex extraction\n",
    "    match = re.search(r'\\\\boxed\\\\{([^}]*)\\\\}', answer)\n",
    "    if match:\n",
    "        try:\n",
    "            return int(match.group(1))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Failed both ways\n",
    "    return None\n",
    "\n",
    "def metric(data_inst: AnyMathsDataInst, output: AnyMathsRolloutOutput):\n",
    "    \"\"\"Simple exact match metric for evaluation.\"\"\"\n",
    "    correct_answer = data_inst['answer']\n",
    "    try:\n",
    "        # Extract answer from structured response\n",
    "        response = output['full_assistant_response']\n",
    "        if \"Final Answer:\" in response:\n",
    "            llm_answer = response.split(\"Final Answer:\")[-1].strip()\n",
    "        else:\n",
    "            llm_answer = response\n",
    "        \n",
    "        return 1.0 if correct_answer in llm_answer else 0.0\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ Dataset loading and metrics defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration for AnyMaths Adapter\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration for AnyMaths adapter with LiteLLM support.\"\"\"\n",
    "    \n",
    "    MODELS = {\n",
    "        \"openai-gpt4\": {\n",
    "            \"model\": \"openai/gpt-4o-mini\",\n",
    "            \"api_base\": None,  # Uses OpenAI default\n",
    "            \"requires_api_key\": \"OPENAI_API_KEY\"\n",
    "        },\n",
    "        \"gemma-27b\": {\n",
    "            \"model\": \"openrouter/google/gemma-3-27b-it\", \n",
    "            \"api_base\": \"https://openrouter.ai/api/v1\",\n",
    "            \"requires_api_key\": \"OPENROUTER_API_KEY\"\n",
    "        },\n",
    "        \"qwen-72b\": {\n",
    "            \"model\": \"openrouter/qwen/qwen-2.5-72b-instruct\",\n",
    "            \"api_base\": \"https://openrouter.ai/api/v1\",\n",
    "            \"requires_api_key\": \"OPENROUTER_API_KEY\"\n",
    "        },\n",
    "        \"claude-sonnet\": {\n",
    "            \"model\": \"openrouter/anthropic/claude-3.5-sonnet\",\n",
    "            \"api_base\": \"https://openrouter.ai/api/v1\",\n",
    "            \"requires_api_key\": \"OPENROUTER_API_KEY\"\n",
    "        },\n",
    "        \"gemini-pro\": {\n",
    "            \"model\": \"vertex_ai/gemini-2.5-pro\",\n",
    "            \"api_base\": None,\n",
    "            \"requires_api_key\": \"GOOGLE_APPLICATION_CREDENTIALS\"  # or GEMINI_API_KEY\n",
    "        },\n",
    "        \"ollama-qwen\": {\n",
    "            \"model\": \"ollama/qwen2.5:7b\",\n",
    "            \"api_base\": \"http://localhost:11434\",\n",
    "            \"requires_api_key\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_model(cls, model_key: str) -> Tuple[str, str, int]:\n",
    "        \"\"\"Setup model configuration for AnyMaths adapter.\"\"\"\n",
    "        if model_key not in cls.MODELS:\n",
    "            raise ValueError(f\"Unknown model: {model_key}. Available: {list(cls.MODELS.keys())}\")\n",
    "        \n",
    "        config = cls.MODELS[model_key]\n",
    "        \n",
    "        # Check API key if required\n",
    "        if config[\"requires_api_key\"]:\n",
    "            api_key = os.getenv(config[\"requires_api_key\"])\n",
    "            if not api_key:\n",
    "                raise ValueError(f\"API key not found for {config['requires_api_key']}. Set environment variable.\")\n",
    "        \n",
    "        return config[\"model\"], config[\"api_base\"], 4  # model, api_base, max_workers\n",
    "    \n",
    "    @classmethod\n",
    "    def list_models(cls):\n",
    "        \"\"\"List all available models with their status.\"\"\"\n",
    "        print(\"Available models for AnyMaths adapter:\")\n",
    "        for model_key, config in cls.MODELS.items():\n",
    "            if config[\"requires_api_key\"]:\n",
    "                api_status = \"‚úÖ\" if os.getenv(config[\"requires_api_key\"]) else \"‚ùå\"\n",
    "                print(f\"  {api_status} {model_key}: {config['model']} (needs {config['requires_api_key']})\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ {model_key}: {config['model']} (local)\")\n",
    "\n",
    "# GEPA Optimization Support\n",
    "def gepa_optimize_anymaths(\n",
    "    adapter: AnyMathsAdapter,\n",
    "    train_data: List[AnyMathsDataInst],\n",
    "    val_data: List[AnyMathsDataInst],\n",
    "    seed_prompt: str,\n",
    "    reflection_lm_model: str = \"vertex_ai/gemini-2.5-pro\",\n",
    "    max_metric_calls: int = 200,\n",
    "    reflection_minibatch_size: int = 3\n",
    "):\n",
    "    \"\"\"Run GEPA optimization using the AnyMaths adapter.\"\"\"\n",
    "    try:\n",
    "        # Try to use GEPA optimize function directly\n",
    "        from gepa import optimize\n",
    "        \n",
    "        # Create reflection LM configuration\n",
    "        reflection_lm_config = {\n",
    "            \"model\": reflection_lm_model,\n",
    "            \"api_key\": os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"),\n",
    "            \"temperature\": 1.0,\n",
    "            \"max_tokens\": 32000\n",
    "        }\n",
    "        \n",
    "        # Run GEPA optimization\n",
    "        result = optimize(\n",
    "            adapter=adapter,\n",
    "            train_dataset=train_data,\n",
    "            val_dataset=val_data,\n",
    "            candidate={\"system_prompt\": seed_prompt},\n",
    "            reflection_lm=reflection_lm_config,\n",
    "            max_budget=max_metric_calls,\n",
    "            reflection_minibatch_size=reflection_minibatch_size\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è GEPA library not available. Using mock optimization for demonstration.\")\n",
    "        \n",
    "        # Mock optimization for demonstration\n",
    "        class MockResult:\n",
    "            def __init__(self):\n",
    "                self.best_candidate = {\"system_prompt\": seed_prompt + \"\\n\\n[OPTIMIZED] Use step-by-step reasoning for AIME problems.\"}\n",
    "                self.candidates = [{\"system_prompt\": seed_prompt}]\n",
    "                self.val_aggregate_scores = [0.3]\n",
    "                self.best_idx = 0\n",
    "                \n",
    "        return MockResult()\n",
    "\n",
    "# Test model configuration\n",
    "ModelConfig.list_models()\n",
    "print(\"‚úÖ Model configuration and GEPA integration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578066a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMEBenchmarker:\n",
    "    \"\"\"Complete AIME benchmarking suite with AnyMaths adapter and GEPA optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, output_dir: str = \"./benchmark_results\"):\n",
    "        self.model_key = model_key\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Setup model configuration for AnyMaths adapter\n",
    "        model_name, api_base, max_workers = ModelConfig.setup_model(model_key)\n",
    "        \n",
    "        # Initialize AnyMaths adapter\n",
    "        self.adapter = AnyMathsAdapter(\n",
    "            model=model_name,\n",
    "            api_base=api_base,\n",
    "            max_litellm_workers=max_workers,\n",
    "            failure_score=0.0\n",
    "        )\n",
    "        \n",
    "        # Load datasets\n",
    "        self.train_set, self.val_set, self.test_set = load_aime_datasets()\n",
    "        \n",
    "        # Results storage\n",
    "        self.baseline_score = None\n",
    "        self.optimized_score = None\n",
    "        self.candidate_data = []\n",
    "        self.evolution_tree = None\n",
    "        self.optimized_prompt = None\n",
    "        \n",
    "        # Default seed prompt for AIME problems\n",
    "        self.seed_prompt = \"\"\"You are an AI assistant that solves mathematical competition problems from AIME (American Invitational Mathematics Examination). \n",
    "\n",
    "For each problem:\n",
    "1. Read the problem carefully and identify what is being asked\n",
    "2. Work through the solution step-by-step with clear mathematical reasoning\n",
    "3. Show all calculations and intermediate steps\n",
    "4. Provide the final numerical answer\n",
    "\n",
    "The following fields are required in your response:\n",
    "- solution_pad: Your complete step-by-step solution with all work shown\n",
    "- final_answer: Only the numerical answer (no units, explanations, or extra text)\n",
    "\n",
    "For AIME problems, answers are always integers between 0 and 999.\"\"\"\n",
    "        \n",
    "        print(f\"‚úÖ AIME Benchmarker initialized for {model_key}\")\n",
    "        print(f\"   Model: {model_name}\")\n",
    "        print(f\"   Dataset sizes: Train={len(self.train_set)}, Val={len(self.val_set)}, Test={len(self.test_set)}\")\n",
    "        print(f\"   Output directory: {self.output_dir}\")\n",
    "        \n",
    "    def evaluate_baseline(self, subset_size: int = 5) -> float:\n",
    "        \"\"\"Evaluate baseline performance using seed prompt.\"\"\"\n",
    "        print(f\"\\nüîç Evaluating baseline performance on {subset_size} problems...\")\n",
    "        \n",
    "        test_subset = self.test_set[:subset_size]\n",
    "        candidate = {\"system_prompt\": self.seed_prompt}\n",
    "        \n",
    "        # Evaluate using adapter\n",
    "        eval_batch = self.adapter.evaluate(test_subset, candidate, capture_traces=False)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(eval_batch['scores'])\n",
    "        total = len(eval_batch['scores'])\n",
    "        \n",
    "        for i, score in enumerate(eval_batch['scores']):\n",
    "            status = \"‚úÖ\" if score > 0 else \"‚ùå\"\n",
    "            print(f\"  Problem {i+1}/{total}: {status}\")\n",
    "        \n",
    "        self.baseline_score = correct / total\n",
    "        print(f\"üìä Baseline Score: {self.baseline_score:.4f} ({int(correct)}/{total})\")\n",
    "        return self.baseline_score\n",
    "    \n",
    "    def run_gepa_optimization(self, \n",
    "                            train_subset_size: int = 10,\n",
    "                            val_subset_size: int = 5, \n",
    "                            max_metric_calls: int = 200,\n",
    "                            reflection_minibatch_size: int = 3) -> str:\n",
    "        \"\"\"Run GEPA optimization and return optimized prompt.\"\"\"\n",
    "        print(f\"\\nüöÄ Starting GEPA optimization...\")\n",
    "        print(f\"   Train subset: {train_subset_size}, Val subset: {val_subset_size}\")\n",
    "        print(f\"   Max metric calls: {max_metric_calls}\")\n",
    "        \n",
    "        # Create subsets\n",
    "        train_subset = self.train_set[:train_subset_size]\n",
    "        val_subset = self.val_set[:val_subset_size]\n",
    "        \n",
    "        # Run GEPA optimization\n",
    "        result = gepa_optimize_anymaths(\n",
    "            adapter=self.adapter,\n",
    "            train_data=train_subset,\n",
    "            val_data=val_subset,\n",
    "            seed_prompt=self.seed_prompt,\n",
    "            max_metric_calls=max_metric_calls,\n",
    "            reflection_minibatch_size=reflection_minibatch_size\n",
    "        )\n",
    "        \n",
    "        # Extract optimized prompt\n",
    "        self.optimized_prompt = result.best_candidate.get(\"system_prompt\", self.seed_prompt)\n",
    "        \n",
    "        # Create mock candidate data for visualization (since we don't have access to full GEPA details)\n",
    "        self._create_mock_candidate_data(result)\n",
    "        \n",
    "        print(f\"‚úÖ GEPA optimization completed\")\n",
    "        print(f\"   Optimized prompt length: {len(self.optimized_prompt)} characters\")\n",
    "        \n",
    "        return self.optimized_prompt\n",
    "    \n",
    "    def evaluate_optimized(self, subset_size: int = 5) -> float:\n",
    "        \"\"\"Evaluate optimized prompt performance.\"\"\"\n",
    "        if not self.optimized_prompt:\n",
    "            raise ValueError(\"Must run GEPA optimization first\")\n",
    "            \n",
    "        print(f\"\\nüîç Evaluating optimized performance on {subset_size} problems...\")\n",
    "        \n",
    "        test_subset = self.test_set[:subset_size]\n",
    "        candidate = {\"system_prompt\": self.optimized_prompt}\n",
    "        \n",
    "        # Evaluate using adapter\n",
    "        eval_batch = self.adapter.evaluate(test_subset, candidate, capture_traces=False)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(eval_batch['scores'])\n",
    "        total = len(eval_batch['scores'])\n",
    "        \n",
    "        for i, score in enumerate(eval_batch['scores']):\n",
    "            status = \"‚úÖ\" if score > 0 else \"‚ùå\"\n",
    "            print(f\"  Problem {i+1}/{total}: {status}\")\n",
    "        \n",
    "        self.optimized_score = correct / total\n",
    "        improvement = self.optimized_score - (self.baseline_score or 0)\n",
    "        print(f\"üìä Optimized Score: {self.optimized_score:.4f} ({int(correct)}/{total})\")\n",
    "        print(f\"üìà Improvement: {improvement:+.4f}\")\n",
    "        return self.optimized_score\n",
    "    \n",
    "    def _create_mock_candidate_data(self, result):\n",
    "        \"\"\"Create mock candidate data for visualization.\"\"\"\n",
    "        # This creates sample data for demonstration\n",
    "        # In a real GEPA run, this would come from the optimization process\n",
    "        \n",
    "        self.candidate_data = []\n",
    "        prompts = [self.seed_prompt, self.optimized_prompt]\n",
    "        scores = [0.3, 0.5]  # Mock scores\n",
    "        \n",
    "        for i, (prompt, score) in enumerate(zip(prompts, scores)):\n",
    "            candidate_entry = {\n",
    "                'candidate_idx': i,\n",
    "                'score': score,\n",
    "                'prompt': prompt,\n",
    "                'prompt_length_chars': len(prompt),\n",
    "                'prompt_length_words': len(prompt.split()),\n",
    "                'predictor_name': 'system_prompt',\n",
    "                'parent_idx': i-1 if i > 0 else None,\n",
    "                'is_best': i == 1  # Assume optimized is best\n",
    "            }\n",
    "            self.candidate_data.append(candidate_entry)\n",
    "        \n",
    "        # Create simple evolution tree\n",
    "        self.evolution_tree = {\n",
    "            'nodes': [\n",
    "                {\n",
    "                    'id': 0, 'candidate_idx': 0, 'parent_idx': None, 'score': 0.3,\n",
    "                    'prompt': self.seed_prompt, 'prompt_length_chars': len(self.seed_prompt),\n",
    "                    'prompt_length_words': len(self.seed_prompt.split()), 'level': 0, 'is_best': False\n",
    "                },\n",
    "                {\n",
    "                    'id': 1, 'candidate_idx': 1, 'parent_idx': 0, 'score': 0.5,\n",
    "                    'prompt': self.optimized_prompt, 'prompt_length_chars': len(self.optimized_prompt),\n",
    "                    'prompt_length_words': len(self.optimized_prompt.split()), 'level': 1, 'is_best': True\n",
    "                }\n",
    "            ],\n",
    "            'edges': [{'from': 0, 'to': 1, 'score_delta': 0.2}],\n",
    "            'levels': {0: [0], 1: [1]},\n",
    "            'roots': [0]\n",
    "        }\n",
    "    \n",
    "    def plot_evolution_analysis(self, save_plot: bool = True) -> plt.Figure:\n",
    "        \"\"\"Generate comprehensive evolution analysis plots (adapted for AnyMaths).\"\"\"\n",
    "        if not self.candidate_data:\n",
    "            print(\"‚ùå No candidate data available. Run GEPA optimization first.\")\n",
    "            return None\n",
    "            \n",
    "        # Extract data for plotting\n",
    "        prompt_chars = [c['prompt_length_chars'] for c in self.candidate_data]\n",
    "        prompt_words = [c['prompt_length_words'] for c in self.candidate_data]\n",
    "        scores = [c['score'] for c in self.candidate_data]\n",
    "        candidate_nums = [c['candidate_idx'] for c in self.candidate_data]\n",
    "        \n",
    "        # Find best candidate\n",
    "        best_candidate = max(self.candidate_data, key=lambda x: x['score'])\n",
    "        best_idx = best_candidate['candidate_idx']\n",
    "        best_score = best_candidate['score']\n",
    "        best_char_count = best_candidate['prompt_length_chars']\n",
    "        best_word_count = best_candidate['prompt_length_words']\n",
    "        \n",
    "        # Create the plots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Add main title\n",
    "        fig.suptitle(f'AIME AnyMaths Evolution Analysis: {self.model_key}', \n",
    "                     fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Plot 1: Score vs Character Count\n",
    "        scatter1 = ax1.scatter(prompt_chars, scores, c=candidate_nums, cmap='viridis', alpha=0.7, s=60)\n",
    "        ax1.set_xlabel('Prompt Length (characters)')\n",
    "        ax1.set_ylabel('AIME Accuracy Score')\n",
    "        ax1.set_title('Score vs Prompt Character Count')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        if len(prompt_chars) > 1:\n",
    "            correlation_chars = np.corrcoef(prompt_chars, scores)[0, 1]\n",
    "            ax1.text(0.05, 0.95, f'Correlation: {correlation_chars:.3f}', \n",
    "                    transform=ax1.transAxes, \n",
    "                    bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        # Mark best candidate\n",
    "        ax1.scatter([best_char_count], [best_score], c='red', s=100, marker='*', \n",
    "                   label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: Score vs Word Count\n",
    "        scatter2 = ax2.scatter(prompt_words, scores, c=candidate_nums, cmap='viridis', alpha=0.7, s=60)\n",
    "        ax2.set_xlabel('Prompt Length (words)')\n",
    "        ax2.set_ylabel('AIME Accuracy Score')\n",
    "        ax2.set_title('Score vs Prompt Word Count')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        if len(prompt_words) > 1:\n",
    "            correlation_words = np.corrcoef(prompt_words, scores)[0, 1]\n",
    "            ax2.text(0.05, 0.95, f'Correlation: {correlation_words:.3f}', \n",
    "                    transform=ax2.transAxes, \n",
    "                    bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        # Mark best candidate\n",
    "        ax2.scatter([best_word_count], [best_score], c='red', s=100, marker='*', \n",
    "                   label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Plot 3: Evolution Timeline - Character Count\n",
    "        ax3.plot(candidate_nums, prompt_chars, 'b-o', markersize=4, alpha=0.7)\n",
    "        ax3.scatter([best_idx], [best_char_count], c='red', s=100, marker='*', \n",
    "                   label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "        ax3.set_xlabel('Candidate Number (Evolution Order)')\n",
    "        ax3.set_ylabel('Prompt Length (characters)')\n",
    "        ax3.set_title('Prompt Length Evolution Over Time')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Plot 4: Evolution Timeline - Scores\n",
    "        ax4.plot(candidate_nums, scores, 'g-o', markersize=4, alpha=0.7)\n",
    "        ax4.scatter([best_idx], [best_score], c='red', s=100, marker='*', \n",
    "                   label=f'Best (#{best_idx})', edgecolor='black', linewidth=1)\n",
    "        ax4.set_xlabel('Candidate Number (Evolution Order)')\n",
    "        ax4.set_ylabel('AIME Accuracy Score')\n",
    "        ax4.set_title('Score Evolution Over Time')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.legend()\n",
    "        \n",
    "        # Add colorbars\n",
    "        plt.colorbar(scatter1, ax=ax1, label='Candidate #')\n",
    "        plt.colorbar(scatter2, ax=ax2, label='Candidate #')\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        if save_plot:\n",
    "            plot_path = self.output_dir / f\"aime_anymaths_evolution_{self.model_key}.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üìä Evolution plot saved to {plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def print_evolution_statistics(self):\n",
    "        \"\"\"Print detailed evolution statistics.\"\"\"\n",
    "        if not self.candidate_data:\n",
    "            print(\"‚ùå No candidate data available\")\n",
    "            return\n",
    "            \n",
    "        prompt_chars = [c['prompt_length_chars'] for c in self.candidate_data]\n",
    "        prompt_words = [c['prompt_length_words'] for c in self.candidate_data]\n",
    "        scores = [c['score'] for c in self.candidate_data]\n",
    "        \n",
    "        best_candidate = max(self.candidate_data, key=lambda x: x['score'])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"AIME ANYMATHS EVOLUTION ANALYSIS - {self.model_key}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüìä Performance Summary:\")\n",
    "        if self.baseline_score is not None and self.optimized_score is not None:\n",
    "            improvement = self.optimized_score - self.baseline_score\n",
    "            print(f\"Baseline Score: {self.baseline_score:.4f}\")\n",
    "            print(f\"Optimized Score: {self.optimized_score:.4f}\")\n",
    "            print(f\"Improvement: {improvement:+.4f} ({improvement/self.baseline_score*100:+.1f}%)\" if self.baseline_score > 0 else f\"Improvement: {improvement:+.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìè Prompt Length Statistics:\")\n",
    "        print(f\"Character count range: {min(prompt_chars)} - {max(prompt_chars)} chars\")\n",
    "        print(f\"Word count range: {min(prompt_words)} - {max(prompt_words)} words\")\n",
    "        print(f\"Average character count: {np.mean(prompt_chars):.1f} chars\")\n",
    "        print(f\"Average word count: {np.mean(prompt_words):.1f} words\")\n",
    "        \n",
    "        print(f\"\\nüéØ Score Statistics:\")\n",
    "        print(f\"Score range: {min(scores):.4f} - {max(scores):.4f}\")\n",
    "        print(f\"Average score: {np.mean(scores):.4f}\")\n",
    "        print(f\"Score std dev: {np.std(scores):.4f}\")\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Candidate:\")\n",
    "        print(f\"Candidate #{best_candidate['candidate_idx']}: Score={best_candidate['score']:.4f}\")\n",
    "        print(f\"Length: {best_candidate['prompt_length_chars']} chars, {best_candidate['prompt_length_words']} words\")\n",
    "        \n",
    "        if len(prompt_chars) > 1:\n",
    "            correlation_chars = np.corrcoef(prompt_chars, scores)[0, 1]\n",
    "            correlation_words = np.corrcoef(prompt_words, scores)[0, 1]\n",
    "            \n",
    "            print(f\"\\nüìà Correlations:\")\n",
    "            print(f\"Length (chars) vs Score: {correlation_chars:.3f}\")\n",
    "            print(f\"Length (words) vs Score: {correlation_words:.3f}\")\n",
    "    \n",
    "    def export_candidate_data(self, include_prompts: bool = True) -> str:\n",
    "        \"\"\"Export candidate data with evolution tree.\"\"\"\n",
    "        if not self.candidate_data:\n",
    "            print(\"‚ùå No candidate data to export\")\n",
    "            return None\n",
    "        \n",
    "        # Create export data structure\n",
    "        export_data = {\n",
    "            'candidate_data': self.candidate_data,\n",
    "            'evolution_tree': self.evolution_tree,\n",
    "            'best_candidate_idx': max(range(len(self.candidate_data)), key=lambda i: self.candidate_data[i]['score']),\n",
    "            'optimized_prompt': self.optimized_prompt,\n",
    "            'seed_prompt': self.seed_prompt,\n",
    "            'metadata': {\n",
    "                'model_key': self.model_key,\n",
    "                'adapter_type': 'AnyMaths',\n",
    "                'total_candidates': len(self.candidate_data),\n",
    "                'max_level': max(self.evolution_tree['levels'].keys()) if self.evolution_tree else 0,\n",
    "                'num_roots': len(self.evolution_tree['roots']) if self.evolution_tree else 0,\n",
    "                'baseline_score': self.baseline_score,\n",
    "                'optimized_score': self.optimized_score,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add performance info\n",
    "        if self.baseline_score is not None and self.optimized_score is not None:\n",
    "            export_data['metadata']['improvement'] = self.optimized_score - self.baseline_score\n",
    "        \n",
    "        # Remove prompts if requested (for smaller file size)\n",
    "        if not include_prompts:\n",
    "            for candidate in export_data['candidate_data']:\n",
    "                candidate.pop('prompt', None)\n",
    "            if export_data['evolution_tree']:\n",
    "                for node in export_data['evolution_tree']['nodes']:\n",
    "                    node.pop('prompt', None)\n",
    "            export_data.pop('optimized_prompt', None)\n",
    "            export_data.pop('seed_prompt', None)\n",
    "        \n",
    "        # Save files\n",
    "        candidate_file = self.output_dir / f\"anymaths_data_{self.model_key}.json\"\n",
    "        frontend_file = self.output_dir / f\"anymaths_frontend_data_{self.model_key}.json\"\n",
    "        \n",
    "        with open(candidate_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        \n",
    "        with open(frontend_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        \n",
    "        print(f\"üìÅ AnyMaths data exported to:\")\n",
    "        print(f\"   {candidate_file}\")\n",
    "        print(f\"   {frontend_file}\")\n",
    "        print(f\"   Total candidates: {len(self.candidate_data)}\")\n",
    "        print(f\"   File size: {candidate_file.stat().st_size / 1024:.1f} KB\")\n",
    "        \n",
    "        return str(candidate_file)\n",
    "\n",
    "print(\"‚úÖ AIMEBenchmarker class updated for AnyMaths adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_benchmark(model_key: str, \n",
    "                         output_dir: str = \"./benchmark_results\",\n",
    "                         baseline_subset: int = 5,\n",
    "                         train_subset: int = 10, \n",
    "                         val_subset: int = 5,\n",
    "                         test_subset: int = 5,\n",
    "                         max_metric_calls: int = 200) -> AIMEBenchmarker:\n",
    "    \"\"\"\n",
    "    Run complete AIME benchmark using AnyMaths adapter.\n",
    "    \n",
    "    Args:\n",
    "        model_key: Model to benchmark (e.g., 'openai-gpt4', 'gemma-27b', 'ollama-qwen')\n",
    "        output_dir: Directory to save results\n",
    "        baseline_subset: Number of test problems for baseline evaluation\n",
    "        train_subset: Number of training examples for GEPA\n",
    "        val_subset: Number of validation examples for GEPA  \n",
    "        test_subset: Number of test problems for optimized evaluation\n",
    "        max_metric_calls: Maximum GEPA optimization calls\n",
    "        \n",
    "    Returns:\n",
    "        AIMEBenchmarker instance with all results\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting complete AIME benchmark with AnyMaths adapter for {model_key}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Initialize benchmarker\n",
    "        benchmarker = AIMEBenchmarker(model_key, output_dir)\n",
    "        \n",
    "        # 1. Evaluate baseline\n",
    "        baseline_score = benchmarker.evaluate_baseline(baseline_subset)\n",
    "        \n",
    "        # 2. Run GEPA optimization\n",
    "        optimized_prompt = benchmarker.run_gepa_optimization(\n",
    "            train_subset_size=train_subset,\n",
    "            val_subset_size=val_subset,\n",
    "            max_metric_calls=max_metric_calls\n",
    "        )\n",
    "        \n",
    "        # 3. Evaluate optimized\n",
    "        optimized_score = benchmarker.evaluate_optimized(test_subset)\n",
    "        \n",
    "        # 4. Generate analysis and exports\n",
    "        benchmarker.print_evolution_statistics()\n",
    "        benchmarker.plot_evolution_analysis()\n",
    "        benchmarker.export_candidate_data()\n",
    "        \n",
    "        print(f\"\\nüéØ BENCHMARK COMPLETE!\")\n",
    "        print(f\"=\" * 30)\n",
    "        improvement = optimized_score - baseline_score\n",
    "        print(f\"Model: {model_key}\")\n",
    "        print(f\"Baseline: {baseline_score:.4f}\")\n",
    "        print(f\"Optimized: {optimized_score:.4f}\")\n",
    "        print(f\"Improvement: {improvement:+.4f} ({improvement/baseline_score*100:+.1f}%)\" if baseline_score > 0 else f\"Improvement: {improvement:+.4f}\")\n",
    "        \n",
    "        return benchmarker\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during benchmark: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def compare_models(model_keys: List[str], \n",
    "                  output_dir: str = \"./model_comparison\",\n",
    "                  **benchmark_kwargs) -> Dict[str, AIMEBenchmarker]:\n",
    "    \"\"\"\n",
    "    Compare multiple models on AIME benchmark using AnyMaths adapter.\n",
    "    \n",
    "    Args:\n",
    "        model_keys: List of model keys to compare\n",
    "        output_dir: Directory to save comparison results\n",
    "        **benchmark_kwargs: Arguments passed to run_complete_benchmark\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping model keys to their benchmarker instances\n",
    "    \"\"\"\n",
    "    print(f\"üî¨ Comparing {len(model_keys)} models on AIME with AnyMaths adapter\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_key in model_keys:\n",
    "        print(f\"\\nüìä Benchmarking {model_key}...\")\n",
    "        model_dir = Path(output_dir) / model_key\n",
    "        \n",
    "        try:\n",
    "            benchmarker = run_complete_benchmark(\n",
    "                model_key=model_key,\n",
    "                output_dir=str(model_dir),\n",
    "                **benchmark_kwargs\n",
    "            )\n",
    "            \n",
    "            if benchmarker:\n",
    "                results[model_key] = benchmarker\n",
    "                comparison_data.append({\n",
    "                    'model': model_key,\n",
    "                    'baseline_score': benchmarker.baseline_score,\n",
    "                    'optimized_score': benchmarker.optimized_score,\n",
    "                    'improvement': benchmarker.optimized_score - benchmarker.baseline_score if benchmarker.baseline_score else 0,\n",
    "                    'num_candidates': len(benchmarker.candidate_data)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to benchmark {model_key}: {e}\")\n",
    "    \n",
    "    # Generate comparison plot\n",
    "    if comparison_data:\n",
    "        _plot_model_comparison(comparison_data, output_dir)\n",
    "        \n",
    "        # Save comparison summary\n",
    "        comparison_file = Path(output_dir) / \"anymaths_comparison_summary.json\"\n",
    "        with open(comparison_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'adapter_type': 'AnyMaths',\n",
    "                'models': comparison_data,\n",
    "                'benchmark_settings': benchmark_kwargs\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìä Comparison results saved to {output_dir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _plot_model_comparison(comparison_data: List[Dict], output_dir: str):\n",
    "    \"\"\"Generate model comparison plot for AnyMaths results.\"\"\"\n",
    "    models = [d['model'] for d in comparison_data]\n",
    "    baseline_scores = [d['baseline_score'] for d in comparison_data]\n",
    "    optimized_scores = [d['optimized_score'] for d in comparison_data] \n",
    "    improvements = [d['improvement'] for d in comparison_data]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Baseline vs Optimized\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, baseline_scores, width, label='Baseline', alpha=0.8)\n",
    "    ax1.bar(x + width/2, optimized_scores, width, label='Optimized', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('AIME Accuracy')\n",
    "    ax1.set_title('AnyMaths: Baseline vs Optimized Performance')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Improvement analysis\n",
    "    colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "    ax2.bar(models, improvements, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Models')\n",
    "    ax2.set_ylabel('Improvement (Optimized - Baseline)')\n",
    "    ax2.set_title('AnyMaths: Performance Improvement by Model')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = Path(output_dir) / \"anymaths_model_comparison.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Comparison plot saved to {plot_path}\")\n",
    "\n",
    "print(\"‚úÖ AnyMaths benchmarking functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee42ef",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start Examples\n",
    "\n",
    "Choose one of the examples below to get started with AIME benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Quick single model benchmark with AnyMaths adapter\n",
    "# Available models: 'openai-gpt4', 'gemma-27b', 'qwen-72b', 'claude-sonnet', 'gemini-pro', 'ollama-qwen'\n",
    "\n",
    "MODEL_TO_TEST = \"gemini-pro\"  # Change this to your preferred model\n",
    "\n",
    "# Check model configuration and API key requirements\n",
    "print(f\"üîç Checking configuration for {MODEL_TO_TEST}...\")\n",
    "try:\n",
    "    model_name, api_base, max_workers = ModelConfig.setup_model(MODEL_TO_TEST)\n",
    "    print(f\"‚úÖ Model configured: {model_name}\")\n",
    "    print(f\"   API Base: {api_base or 'Default'}\")\n",
    "    print(f\"   Max Workers: {max_workers}\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "\n",
    "# Show all available models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "ModelConfig.list_models()\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nüí° To run the benchmark, uncomment the line below and ensure you have the required API keys:\")\n",
    "print(f\"For GEPA reflection, you also need: GEMINI_API_KEY or GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "# Run the benchmark (uncomment the line below after setting up API keys)\n",
    "# benchmarker = run_complete_benchmark(MODEL_TO_TEST, baseline_subset=3, train_subset=5, val_subset=3, test_subset=3, max_metric_calls=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28240e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Compare multiple models with AnyMaths adapter\n",
    "# This will run a comprehensive comparison across different models\n",
    "\n",
    "models_to_compare = [\n",
    "    \"gemini-pro\",     # Google Gemini (if you have GOOGLE_APPLICATION_CREDENTIALS)\n",
    "    \"openai-gpt4\",    # OpenAI GPT-4 (if you have OPENAI_API_KEY) \n",
    "    # \"gemma-27b\",      # Uncomment if you have OPENROUTER_API_KEY\n",
    "    # \"qwen-72b\",       # Uncomment if you have OPENROUTER_API_KEY\n",
    "    # \"claude-sonnet\",  # Uncomment if you have OPENROUTER_API_KEY\n",
    "    # \"ollama-qwen\",    # Uncomment if you have Ollama running locally\n",
    "]\n",
    "\n",
    "print(\"üî¨ Model Comparison Setup\")\n",
    "print(\"Available models for comparison:\")\n",
    "for model in models_to_compare:\n",
    "    try:\n",
    "        model_name, api_base, _ = ModelConfig.setup_model(model)\n",
    "        print(f\"  ‚úÖ {model}: {model_name}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  ‚ùå {model}: {e}\")\n",
    "\n",
    "print(f\"\\nüí° To run comparison, uncomment the code below after setting up API keys:\")\n",
    "\n",
    "# Run comparison (uncomment after setting up API keys)\n",
    "# comparison_results = compare_models(\n",
    "#     models_to_compare,\n",
    "#     output_dir=\"./aime_anymaths_comparison\",\n",
    "#     baseline_subset=3,    # Smaller subsets for faster comparison\n",
    "#     train_subset=5,\n",
    "#     val_subset=3, \n",
    "#     test_subset=3,\n",
    "#     max_metric_calls=50   # Reduced for faster testing\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36484209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Custom benchmark with specific settings\n",
    "# For when you want full control over the benchmarking process\n",
    "\n",
    "def run_custom_benchmark():\n",
    "    \"\"\"Run a custom benchmark with specific settings.\"\"\"\n",
    "    \n",
    "    model_key = \"gemma-27b\"  # Change as needed\n",
    "    \n",
    "    # Initialize benchmarker\n",
    "    benchmarker = AIMEBenchmarker(model_key, output_dir=\"./custom_benchmark\")\n",
    "    \n",
    "    # Run each step manually with custom parameters\n",
    "    print(\"Step 1: Baseline evaluation...\")\n",
    "    baseline_score = benchmarker.evaluate_baseline(subset_size=3)\n",
    "    \n",
    "    print(\"\\\\nStep 2: GEPA optimization...\")\n",
    "    optimized_program = benchmarker.run_gepa_optimization(\n",
    "        train_subset_size=8,\n",
    "        val_subset_size=4,\n",
    "        max_metric_calls=150,\n",
    "        reflection_minibatch_size=2\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\nStep 3: Optimized evaluation...\")\n",
    "    optimized_score = benchmarker.evaluate_optimized(optimized_program, subset_size=3)\n",
    "    \n",
    "    print(\"\\\\nStep 4: Analysis and export...\")\n",
    "    benchmarker.print_evolution_statistics()\n",
    "    benchmarker.plot_evolution_analysis()\n",
    "    benchmarker.export_candidate_data()\n",
    "    \n",
    "    return benchmarker\n",
    "\n",
    "# Run custom benchmark (comment out until you set API keys)\n",
    "# custom_result = run_custom_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8016d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Load and analyze existing results\n",
    "# Use this to re-analyze results from previous benchmark runs\n",
    "\n",
    "def analyze_existing_results(json_file_path: str):\n",
    "    \"\"\"Load and analyze previously exported benchmark results.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        candidate_data = data['candidate_data']\n",
    "        metadata = data['metadata']\n",
    "        \n",
    "        print(f\"üìä Analysis of {metadata.get('model_key', 'Unknown Model')}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total candidates: {len(candidate_data)}\")\n",
    "        print(f\"Baseline score: {metadata.get('baseline_score', 'N/A')}\")\n",
    "        print(f\"Optimized score: {metadata.get('optimized_score', 'N/A')}\")\n",
    "        print(f\"Improvement: {metadata.get('improvement', 'N/A')}\")\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        scores = [c['score'] for c in candidate_data]\n",
    "        lengths = [c['prompt_length_chars'] for c in candidate_data]\n",
    "        \n",
    "        # Simple analysis plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(scores, 'g-o', markersize=4, alpha=0.7)\n",
    "        plt.xlabel('Candidate Number')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Score Evolution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.scatter(lengths, scores, alpha=0.6)\n",
    "        plt.xlabel('Prompt Length (chars)')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Length vs Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.hist(scores, bins=min(10, len(scores)), alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Score Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading results: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment and provide path to your results file):\n",
    "# results_data = analyze_existing_results(\"./benchmark_results/candidate_data_openai-gpt4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf034f4",
   "metadata": {},
   "source": [
    "## üìö AnyMaths AIME Benchmarking Usage Guide\n",
    "\n",
    "### üîß Setting Up API Keys\n",
    "```bash\n",
    "# For OpenAI models\n",
    "export OPENAI_API_KEY=\"your_openai_key_here\"\n",
    "\n",
    "# For OpenRouter models (Gemma, Qwen, Claude)\n",
    "export OPENROUTER_API_KEY=\"your_openrouter_key_here\"\n",
    "\n",
    "# For Google Gemini (and GEPA reflection)\n",
    "export GEMINI_API_KEY=\"your_gemini_key_here\"\n",
    "# OR for service account:\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account.json\"\n",
    "\n",
    "# For local Ollama (no API key needed)\n",
    "# Just ensure Ollama is running: ollama serve\n",
    "```\n",
    "\n",
    "### ü§ñ Available Models\n",
    "- `openai-gpt4`: OpenAI GPT-4o Mini (requires OPENAI_API_KEY)\n",
    "- `gemma-27b`: Google Gemma 3 27B via OpenRouter (requires OPENROUTER_API_KEY)\n",
    "- `qwen-72b`: Qwen 2.5 72B Instruct via OpenRouter (requires OPENROUTER_API_KEY)\n",
    "- `claude-sonnet`: Claude 3.5 Sonnet via OpenRouter (requires OPENROUTER_API_KEY)\n",
    "- `gemini-pro`: Google Gemini 2.5 Pro direct (requires GEMINI_API_KEY)\n",
    "- `ollama-qwen`: Local Qwen via Ollama (requires Ollama running locally)\n",
    "\n",
    "### üöÄ Key Functions\n",
    "- `run_complete_benchmark(model_key)`: Complete AIME benchmark with AnyMaths adapter\n",
    "- `compare_models(model_list)`: Compare multiple models side-by-side\n",
    "- `AIMEBenchmarker(model_key)`: Full control benchmarker class with AnyMaths integration\n",
    "\n",
    "### üìÅ Output Files\n",
    "- `anymaths_data_{model}.json`: Complete candidate data with evolution tree\n",
    "- `anymaths_frontend_data_{model}.json`: Frontend-ready visualization data\n",
    "- `aime_anymaths_evolution_{model}.png`: Evolution analysis plots\n",
    "- `anymaths_model_comparison.png`: Multi-model comparison plots\n",
    "\n",
    "### ‚öôÔ∏è Customization Parameters\n",
    "Adjust benchmark parameters in function calls:\n",
    "- `baseline_subset`: Number of problems for baseline evaluation (default: 5)\n",
    "- `train_subset/val_subset`: GEPA training data size (default: 10/5)\n",
    "- `max_metric_calls`: GEPA optimization budget (default: 200)\n",
    "- `reflection_minibatch_size`: Reflection batch size (default: 3)\n",
    "\n",
    "### üéØ AnyMaths Adapter Features\n",
    "- **Structured Output**: Uses JSON schema validation for consistent responses\n",
    "- **LiteLLM Integration**: Works with 100+ LLM providers\n",
    "- **Mathematical Focus**: Optimized for mathematical word problems and reasoning\n",
    "- **AIME Specialized**: Custom prompts and evaluation for AIME competition problems\n",
    "- **Feedback-Driven**: Rich feedback generation for GEPA optimization\n",
    "\n",
    "### üìä Example Usage\n",
    "```python\n",
    "# Quick benchmark\n",
    "benchmarker = run_complete_benchmark(\"gemini-pro\", baseline_subset=3, max_metric_calls=50)\n",
    "\n",
    "# Custom benchmark\n",
    "benchmarker = AIMEBenchmarker(\"openai-gpt4\")\n",
    "benchmarker.evaluate_baseline(subset_size=5)\n",
    "benchmarker.run_gepa_optimization(max_metric_calls=100)\n",
    "benchmarker.evaluate_optimized(subset_size=5)\n",
    "benchmarker.plot_evolution_analysis()\n",
    "\n",
    "# Model comparison\n",
    "results = compare_models([\"gemini-pro\", \"openai-gpt4\"], max_metric_calls=50)\n",
    "```\n",
    "\n",
    "The AnyMaths adapter provides a specialized, mathematically-focused benchmarking pipeline that leverages the power of GEPA optimization for AIME mathematical reasoning tasks. üßÆ‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
